{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./fr-en/europarl-v7.fr-en.en\",mode='r',encoding='utf-8') as f:\n",
    "    europarl_en = f.read()\n",
    "    \n",
    "with open(\"./fr-en/europarl-v7.fr-en.fr\",mode='r',encoding='utf-8') as f:\n",
    "    europarl_fr = f.read()\n",
    "    \n",
    "with open(\"./fr-en/nonbreaking_prefix.en\",mode='r',encoding='utf-8') as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "\n",
    "with open(\"./fr-en/nonbreaking_prefix.fr\",mode='r',encoding='utf-8') as f:\n",
    "    non_breaking_prefix_fr = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the se'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_en[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_breaking_prefix_en = non_breaking_prefix_en.split('\\n')\n",
    "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
    "\n",
    "non_breaking_prefix_fr = non_breaking_prefix_fr.split('\\n')\n",
    "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces, so we can \"tokenize\" them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en = europarl_en\n",
    "for prefix in non_breaking_prefix_en:\n",
    "    corpus_en = corpus_en.replace(prefix, prefix + \"###\")\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_en)\n",
    "corpus_en = re.sub(r\"\\.###\",'',corpus_en)\n",
    "corpus_en = re.sub(r\" +\", ' ', corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "corpus_fr = europarl_fr\n",
    "for prefix in non_breaking_prefix_fr:\n",
    "    corpus_fr = corpus_fr.replace(prefix, prefix + \"###\")\n",
    "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_fr)\n",
    "corpus_fr = re.sub(r\"\\.###\",'',corpus_fr)\n",
    "corpus_fr = re.sub(r\" +\", ' ', corpus_fr)\n",
    "corpus_fr = corpus_fr.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_en, target_vocab_size=2**13)\n",
    "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_fr, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
    "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
    "          for sentence in corpus_en]\n",
    "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
    "           for sentence in corpus_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove too long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs/outputs creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
